{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "855d49b7",
   "metadata": {},
   "source": [
    "## Student Name: Edmundo Fernandes\n",
    "\n",
    "## Student Number: sbs 23034\n",
    "\n",
    "## Class Module: Advanced Data & Big Data\n",
    "\n",
    "## Lecturer Name: David McQuaid & Muhammad Iqbal\n",
    "\n",
    "## Date of Submission:\n",
    "\n",
    "## Github Link: https://github.com/Young-Jedi79/Integrated_CA2-MSc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007717d5",
   "metadata": {},
   "source": [
    "### Data Preparation And Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ec3c3",
   "metadata": {},
   "source": [
    "#### Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf3eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.ticker as ticker\n",
    "import scipy as sc\n",
    "import matplotlib.animation as animation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.models import ColumnDataSource, FactorRange, HoverTool\n",
    "from bokeh.transform import factor_cmap\n",
    "from dateutil.parser import parse \n",
    "import matplotlib.font_manager as fm\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.stattools import adfuller, acf,pacf\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARMA\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "\n",
    "import gensim\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_log_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import plotly.express as ex\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo\n",
    "from plotly.subplots import make_subplots\n",
    "pyo.init_notebook_mode()\n",
    "from sklearn.decomposition import TruncatedSVD,PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('vader_lexicon')\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import random\n",
    "plt.rc('figure',figsize=(17,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd995c0",
   "metadata": {},
   "source": [
    "### Dataset details:\n",
    "\n",
    "ids: unique id of the tweet\n",
    "\n",
    "date:date of the tweet\n",
    "\n",
    "flag:refers to the query. If no query exists, then it is NO QUERY\n",
    "\n",
    "user:It refers to the name of the user that tweeted\n",
    "\n",
    "text:refers to the name of the user that tweeted\n",
    "\n",
    "sentiment:polarity of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fef567",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ProjectTweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537a2806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029c2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total tweets in the dataset: {} Million\".format(df.shape[0]/1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047297b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6476c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa34cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(df.columns)\n",
    "df = df.iloc[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(title='ProjectTweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3ff742",
   "metadata": {},
   "source": [
    "I will rename the columns for a clearer visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99803e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'0': 'index', '1467810369':'ids', 'Mon Apr 06 22:19:45 PDT 2009': 'date', 'NO_QUERY': 'flag',\n",
    "                    '_TheSpecialOne_':'user', \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\":'text'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca1ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('length of data is', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae17fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a66a46",
   "metadata": {},
   "source": [
    "### Number of missing values per feature\n",
    "We will now check how missing values are in our remaining features and then decide whether to drop them or replace them with a meaningful value.\n",
    "\n",
    "### Missing Values:\n",
    "By calling the .isnull().sum(), isnull().mean(), value_counts() and .describe() functions we can check the number of standard missing values in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dbc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_missing = ((df.isnull().sum()).sort_values(ascending=True))\n",
    "count_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143bb68",
   "metadata": {},
   "source": [
    "### Rows and columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d520fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count of columns in the data is:  ', len(df.columns))\n",
    "print('Count of rows in the data is:  ', len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965b6b2",
   "metadata": {},
   "source": [
    "### Number of distinct values in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe27dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col, ': Number of Distinct Values: ', len(df[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde1d6a",
   "metadata": {},
   "source": [
    "### Print the distinct values in each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa10ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df:\n",
    "    print('COLUMN NAME: ', col,':')\n",
    "    print(df[col].unique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd5e85",
   "metadata": {},
   "source": [
    "I will now drop the column index as there is no need for my findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab022bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['index', 'flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334eb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af95029",
   "metadata": {},
   "source": [
    "As the date column is wrongly set as object type, let's change it to Datetime type in the following order Year, Month, Day, Hour, Minute, Second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d071db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(df['date'].astype(str), format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c8d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.min(),df.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797365dd",
   "metadata": {},
   "source": [
    "I will compute the amount of days between the limits in the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdc345",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.max()-df.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ce5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c1534",
   "metadata": {},
   "source": [
    "I will visualize how many Qualitative features are there in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81af34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative=[feature for feature in df.columns if len(df[feature].unique())<=10]\n",
    "qualitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af680c7",
   "metadata": {},
   "source": [
    "As we can see, there is 1 qualitative feature in the dataset\n",
    "\n",
    "I will now analise the amount of unique values in the qualitative features\n",
    "\n",
    "Quantitative data represents numerical data and qualitative represents Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813521f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in qualitative:\n",
    "    print(feature)\n",
    "    print(df[feature].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea1746",
   "metadata": {},
   "source": [
    "Flag feature has 1 unique qualitative value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc368c",
   "metadata": {},
   "source": [
    "The following code tells me the amount of quantitative variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative=[feature for feature in df.columns if df[feature].dtype!='O' and\n",
    "             len(df[feature].unique())>10]\n",
    "quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d165f",
   "metadata": {},
   "source": [
    "I will now use Histogram to visualize the distribution of quantitative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436776d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in quantitative:\n",
    "    sns.histplot(df[feature])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15786f",
   "metadata": {},
   "source": [
    "Lets visualize the distribution between the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f91c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=50, sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12,12))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a528ab",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "\n",
    "Top 10 tweets - Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "ax = df['text'].value_counts().nlargest(10).plot(kind='pie', fontsize=14, autopct='%1.1f%%', legend = True, label='')\n",
    "ax.set_title(\"Tweets - Top 10\", fontsize=20, weight='bold')\n",
    "plt.legend(loc=\"best\", bbox_to_anchor=(1.3, 0.5, 0.5, 0.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d85ce6",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d589ed",
   "metadata": {},
   "source": [
    "### Statistical Summary of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49366d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01209b",
   "metadata": {},
   "source": [
    "#### Seaborn Pairplot (all numerical features):\n",
    "\n",
    "In order to create our scatterplot pairplot, we use the .pairplot() function from the seaborn library (Waskom, n.d.b). We can clearly see that there are no linear relationships between the numerical features in our DataFrame in the pairplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe778024",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,12))\n",
    "g = sns.pairplot(df)\n",
    "g.fig.suptitle('Pairplot (Numerical Features) - Tweets in 2009\\n',\n",
    "               color='red',\n",
    "               fontsize=25,\n",
    "               fontweight='bold',\n",
    "               y=1.04)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eff00a",
   "metadata": {},
   "source": [
    "### Statistical Summary of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25749d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=object).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800b4c6",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c066da",
   "metadata": {},
   "source": [
    "Now I will consider the first tweet for sentiment analysis, taking into cosideration that inside a tweet, there are several categories, such as user, text, emojis, links and user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3210c3b",
   "metadata": {},
   "source": [
    "For analysis, I will preprocess the text to analyse the way the model is trained.\n",
    "the way the tweeter model is used is by using @user to mention a user and instead of hyperlink, just write http"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70966afd",
   "metadata": {},
   "source": [
    "### Preprocess tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296adfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303095a",
   "metadata": {},
   "source": [
    "It appears that there are a lot of elements in the tweets that we can take a look at. There are usernames, urls, emoticons, extra dots, exclamation marks and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in tweet.split(' '):\n",
    "    if word.startswith('@') and len(word) >1:\n",
    "        word = '@user'\n",
    "        \n",
    "    elif word.startswith('http'):\n",
    "        word = 'http'\n",
    "    tweet_words.append(word)\n",
    "    \n",
    "print(tweet_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8429786",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_proc = ' '.join(tweet_words)\n",
    "print(tweet_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280507e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'cardiffnlp/twitter-roberta-base-sentiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb89ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification. from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24b3a0b",
   "metadata": {},
   "source": [
    "Now I will make a list of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb89db",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Negative', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da015e6",
   "metadata": {},
   "source": [
    "Now I will write the coode for tweet sentiment analysis\n",
    "\n",
    "I will convert the tweet into pytorch tensors and then pass into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93005c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tweet = tokenizer(tweet_proc, \n",
    "                          return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c4ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691c75f",
   "metadata": {},
   "source": [
    "We can see that the encoded tweet is a dictionary with the tensor and attention mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339bbe1",
   "metadata": {},
   "source": [
    "Now I will pass the encoded tweet into the model to do the sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(encoded_tweet['input_ids'],\n",
    "              encoded_tweet['attention_mask'])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886b376",
   "metadata": {},
   "source": [
    "I got the probabilities and now will print the output with corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f280a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scores)):\n",
    "    \n",
    "    l = labels[i]\n",
    "    s = scores[i]\n",
    "    print(l,s)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebde19",
   "metadata": {},
   "source": [
    "I will now use a prebuilt sentiment analysis tool in this case TextBlob to perform sentiment analysis in the column text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306c9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eab543",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['text'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2f795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe31f3",
   "metadata": {},
   "source": [
    "Lets analyse the new column sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29209da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['sentiment'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef2e2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3560824",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['sentiment'], bins=20, kde=True)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fa2ef",
   "metadata": {},
   "source": [
    "Accordint to this results, we can see that the highest amounts of tweets are 0 or neutral, but further analysis will investigate that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002bf9e2",
   "metadata": {},
   "source": [
    "I will now define a function to categorize the column sentiment into positive, negative and neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ac97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['sentiment'].apply(lambda x: 'positive' if x > 0 else ('neutral' if x == 0 else 'negative'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c29d634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0662c5",
   "metadata": {},
   "source": [
    "Lets see 1 tweet for each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f526f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"category\"] == 'positive'].head(1))\n",
    "display(df[df[\"category\"] == 'neutral'].head(1))\n",
    "display(df[df[\"category\"] == 'negative'].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06830511",
   "metadata": {},
   "source": [
    "Next, I will count the number of tweets that are positive, negative and neutral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = df.groupby(['category']).size()\n",
    "print(sentiment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f653f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29ff3b",
   "metadata": {},
   "source": [
    "We can see that most tweets are positive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f410d",
   "metadata": {},
   "source": [
    "Lets use a box plot to understand the distribution and the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb65333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('category', axis=1).plot(kind='box', subplots=True, layout=(6,2), sharex=False,\n",
    "sharey=False, figsize=(10,10), title='Box Plot for numerical variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53d9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cnt = Counter(df.category)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.bar(target_cnt.keys(), target_cnt.values())\n",
    "plt.title(\"Dataset labels distribuition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa44e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = df['category'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "tick_locations = [0, 1, 2]\n",
    "tick_labels = ['positive', 'neutral', 'negative']\n",
    "plt.xticks(tick_locations, tick_labels)\n",
    "plt.bar(x=tick_locations, height=class_count.values, color=['g', 'r', 'b'])\n",
    "plt.xlabel('Sentiment Categories')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiment Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98057df",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1f9e33",
   "metadata": {},
   "source": [
    "I will now use stop words to filter out common and non-informative words from text data.\n",
    "\n",
    "These words are words considered to have little or no value in terms of the meaning of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b466c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text =df.text.str.lower()\n",
    "\n",
    "#Remove twitter handlers\n",
    "df.text = df.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n",
    "\n",
    "#remove hashtags\n",
    "df.text = df.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n",
    "\n",
    "\n",
    "# Remove URLS\n",
    "df.text = df.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "# Remove all the special characters\n",
    "df.text = df.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n",
    "\n",
    "#remove all single characters\n",
    "df.text = df.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n",
    "\n",
    "# Substituting multiple spaces with single space\n",
    "df.text = df.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93cfa48",
   "metadata": {},
   "source": [
    "### Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cf4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SIA()\n",
    "df['sentiments']           = df['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\n",
    "df['Positive Sentiment']   = df['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \n",
    "df['Neutral Sentiment']    = df['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\n",
    "df['Negative Sentiment']   = df['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n",
    "\n",
    "df.drop(columns=['sentiments'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5a5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Words\n",
    "df['Number_Of_Words'] = df.text.apply(lambda x:len(x.split(' ')))\n",
    "#Average Word Length\n",
    "df['Mean_Word_Length'] = df.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c341390",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(df['Negative Sentiment'],bw=0.1)\n",
    "sns.kdeplot(df['Positive Sentiment'],bw=0.1)\n",
    "sns.kdeplot(df['Neutral Sentiment'],bw=0.1)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(df['Negative Sentiment'],bw=0.1,cumulative=True)\n",
    "sns.kdeplot(df['Positive Sentiment'],bw=0.1,cumulative=True)\n",
    "sns.kdeplot(df['Neutral Sentiment'],bw=0.1,cumulative=True)\n",
    "plt.xlabel('Sentiment Value',fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c6ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Words\n",
    "df['Number_Of_Words'] = df.text.apply(lambda x:len(x.split(' ')))\n",
    "#Average Word Length\n",
    "df['Mean_Word_Length'] = df.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b856db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50663c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.title('Distriubtion Of Sentiments Across Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(df['Negative Sentiment'],bw=0.1)\n",
    "sns.kdeplot(df['Positive Sentiment'],bw=0.1)\n",
    "sns.kdeplot(df['Neutral Sentiment'],bw=0.1)\n",
    "plt.subplot(2,1,2)\n",
    "plt.title('Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\n",
    "sns.kdeplot(df['Negative Sentiment'],bw=0.1,cumulative=True)\n",
    "sns.kdeplot(df['Positive Sentiment'],bw=0.1,cumulative=True)\n",
    "sns.kdeplot(df['Neutral Sentiment'],bw=0.1,cumulative=True)\n",
    "plt.xlabel('Sentiment Value',fontsize=19)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f253f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1a8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d5882",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='date')\n",
    "df['date'] = pd.to_datetime(df['date']).dt.date\n",
    "\n",
    "# Extract various date components\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "df['day_of_year'] = pd.DatetimeIndex(df['date']).dayofyear\n",
    "df['quarter'] = pd.DatetimeIndex(df['date']).quarter\n",
    "df['season'] = df.month % 12 // 3 + 1\n",
    "\n",
    "# Add 1 week, 1 month, and 3 months\n",
    "df['date_1_week_later'] = df['date'] + pd.DateOffset(weeks=1)\n",
    "df['date_1_month_later'] = df['date'] + pd.DateOffset(months=1)\n",
    "df['date_3_months_later'] = df['date'] + pd.DateOffset(months=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f4b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c819d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index().drop(columns=['index'])\n",
    "partitions = []\n",
    "partitions.append(df.loc[44:np.round(len(df)/3,0)-1,:])\n",
    "partitions.append(df.loc[np.round(len(df)/3,0):2*int(len(df)/3)-1,:])\n",
    "partitions.append(df.loc[2*np.round(len(df)/3,0):3*int(len(df)/3)-1,:])\n",
    "\n",
    "\n",
    "\n",
    "neg_part_means =[]\n",
    "neg_part_std   =[]\n",
    "pos_part_means =[]\n",
    "pos_part_std   =[]\n",
    "for part in partitions:\n",
    "    neg_part_means.append(part['Negative Sentiment'].mean())\n",
    "    neg_part_std.append(part['Negative Sentiment'].std())\n",
    "    pos_part_means.append(part['Positive Sentiment'].mean())\n",
    "    pos_part_std.append(part['Positive Sentiment'].std())\n",
    "    \n",
    "res_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n",
    "                     index = [f'Partition_{i}' for i in range(1,4)])\n",
    "\n",
    "\n",
    "def highlight_greater(x):\n",
    "    temp = x.copy()\n",
    "    temp = temp.round(0).astype(int)\n",
    "    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n",
    "    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n",
    "    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n",
    "    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n",
    "\n",
    "    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n",
    "    #rewrite values by boolean masks\n",
    "    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n",
    "    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n",
    "    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n",
    "\n",
    "    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n",
    "    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n",
    "    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "\n",
    "#res_df.style.apply(highlight_greater,axis=None)\n",
    "res_df = res_df.T\n",
    "res_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\n",
    "res_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\n",
    "res_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\n",
    "res_df.style.apply(highlight_greater,axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b5f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "partitions.append(df.loc[44:np.round(len(df) / 3, 0) - 1, :])\n",
    "partitions.append(df.loc[np.round(len(df) / 3, 0):2 * int(len(df) / 3) - 1, :])\n",
    "partitions.append(df.loc[2 * np.round(len(df) / 3, 0):3 * int(len(df) / 3) - 1, :])\n",
    "\n",
    "neg_part_means = []\n",
    "neg_part_std = []\n",
    "pos_part_means = []\n",
    "pos_part_std = []\n",
    "for part in partitions:\n",
    "    neg_part_means.append(part['Negative Sentiment'].mean())\n",
    "    neg_part_std.append(part['Negative Sentiment'].std())\n",
    "    pos_part_means.append(part['Positive Sentiment'].mean())\n",
    "    pos_part_std.append(part['Positive Sentiment'].std())\n",
    "\n",
    "res_df = pd.DataFrame({'Positive Sentiment Mean': pos_part_means,\n",
    "                       'Negative Sentiment Mean': neg_part_means,\n",
    "                       'Positive Sentiment SD': pos_part_std,\n",
    "                       'Negative Sentiment SD': neg_part_std},\n",
    "                      index=[f'Partition_{i}' for i in range(1, 4)])\n",
    "\n",
    "def highlight_greater(x):\n",
    "    temp = x.copy()\n",
    "    temp = temp.round(0).astype(int)\n",
    "    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n",
    "    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n",
    "    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean'] + 3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean'] - 3)\n",
    "    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD'] + 3) & (temp['Partition_1_SD'] > temp['Partition_2_SD'] - 3)\n",
    "\n",
    "    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n",
    "    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: salmon', df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: salmon', df1['Partition_2_Mean'])\n",
    "    df1['Partition_1_Mean'] = np.where(m3, 'background-color: gold', df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(m3, 'background-color: gold', df1['Partition_2_Mean'])\n",
    "    df1['Partition_1_Mean'] = np.where(m1, 'background-color: mediumseagreen', df1['Partition_1_Mean'])\n",
    "    df1['Partition_2_Mean'] = np.where(m1, 'background-color: mediumseagreen', df1['Partition_2_Mean'])\n",
    "\n",
    "    df1['Partition_1_SD'] = np.where(~m2, 'background-color: salmon', df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(~m2, 'background-color: salmon', df1['Partition_2_SD'])\n",
    "    df1['Partition_1_SD'] = np.where(m4, 'background-color: gold', df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(m4, 'background-color: gold', df1['Partition_2_SD'])\n",
    "    df1['Partition_1_SD'] = np.where(m2, 'background-color: mediumseagreen', df1['Partition_1_SD'])\n",
    "    df1['Partition_2_SD'] = np.where(m2, 'background-color: mediumseagreen', df1['Partition_2_SD'])\n",
    "\n",
    "    return df1\n",
    "\n",
    "res_df = res_df.T\n",
    "res_df = pd.DataFrame(res_df.values, columns=res_df.columns,\n",
    "                      index=['Positive Sentiment', 'Negative Sentiment', 'Positive Sentiment', 'Negative Sentiment'])\n",
    "res_df = pd.concat([res_df.iloc[:2, :], res_df.iloc[2:, :]], axis=1)\n",
    "res_df.columns = ['Partition_1_Mean', 'Partition_2_Mean', 'Partition_3_Mean', 'Partition_1_SD', 'Partition_2_SD', 'Partition_3_SD']\n",
    "res_df.style.apply(highlight_greater, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca765975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff1c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2)\n",
    "\n",
    "for idx,prt in enumerate(partitions):\n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=prt['date'], y=prt['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n",
    "    row=idx+1, col=1)\n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=prt['date'], y=prt['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n",
    "    row=idx+1, col=2)\n",
    "\n",
    "fig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Sentiments Over Our Time Line For Each Partition\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c511bcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49be881",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=2)\n",
    "\n",
    "for idx, prt in enumerate(partitions):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=prt['date'], y=prt['Positive Sentiment'], name=f'Positive Part {idx + 1}'),\n",
    "        row=idx + 1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=prt['date'], y=prt['Negative Sentiment'], name=f'Negative Part {idx + 1}'),\n",
    "        row=idx + 1, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=600, width=900, title_text=\"Distribution Of Sentiments Over Time Line For Each Partition\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc6353f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db47231",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\n",
    "b_date_mean = df.groupby(by='date').mean().reset_index()\n",
    "\n",
    "lbl = ['Positive','Negative']\n",
    "\n",
    "for idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n",
    "    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n",
    "    \n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n",
    "    row=1, col=idx+1)\n",
    "    \n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n",
    "    row=2, col=idx+1)\n",
    "    \n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n",
    "    row=3, col=idx+1)\n",
    "    \n",
    "    fig.add_trace(\n",
    "    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n",
    "    row=4, col=idx+1)\n",
    "            \n",
    "fig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077db0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n",
    "\n",
    "ax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\n",
    "autocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\n",
    "ax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\n",
    "autocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a886d84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d30ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n",
    "ax[0,0].set_ylim(-1.1,1.1)\n",
    "ax[1,0].set_ylim(-1.1,1.1)\n",
    "ax[0,1].set_ylim(-1.1,1.1)\n",
    "ax[1,1].set_ylim(-1.1,1.1)\n",
    "\n",
    "plot_acf(b_date_mean['Negative Sentiment'],lags=20, ax=ax[0,0],title='Autocorrelation Negative')\n",
    "plot_pacf(b_date_mean['Negative Sentiment'],lags=20, ax=ax[1,0],title='Partial Autocorrelation Negative')\n",
    "plot_acf(b_date_mean['Positive Sentiment'],lags=20, ax=ax[0,1],color='tab:red',title='Autocorrelation Positive')\n",
    "plot_pacf(b_date_mean['Positive Sentiment'],lags=20, ax=ax[1,1],color='tab:red',title='Partial Autocorrelation Positive')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c624f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e414b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_date_mean = df.groupby(by='date').mean().reset_index()\n",
    "b_date_std = df.groupby(by='date').std().reset_index()\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "    \n",
    "#positive mean\n",
    "fig.add_shape(type=\"line\",\n",
    "    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n",
    "    line=dict(\n",
    "        color=\"Red\",\n",
    "        width=2,\n",
    "        dash=\"dashdot\",\n",
    "    ),\n",
    "        name='Mean'\n",
    ")\n",
    "\n",
    "fig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Positive Sentiment'].mean(),\n",
    "            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Positive Sentiment'].mean()),\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            yshift=10)\n",
    "\n",
    "\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "#negative mean\n",
    "fig.add_shape(type=\"line\",\n",
    "    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n",
    "    line=dict(\n",
    "        color=\"Red\",\n",
    "        width=2,\n",
    "        dash=\"dashdot\",\n",
    "    ),\n",
    "        name='Mean',\n",
    "        xref='x2', \n",
    "        yref='y2'\n",
    ")\n",
    "\n",
    "fig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Negative Sentiment'].mean(),\n",
    "            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Negative Sentiment'].mean()),\n",
    "            showarrow=True,\n",
    "            arrowhead=3,\n",
    "            yshift=10,\n",
    "            xref='x2', \n",
    "            yref='y2')\n",
    "\n",
    "\n",
    "\n",
    "fig.add_annotation(x=b_date_mean['date'].values[5], y=b_date_mean['Negative Sentiment'].mean()+0.01,\n",
    "            text=r\"Start Of Decline\",\n",
    "            showarrow=True,\n",
    "            arrowhead=6,\n",
    "            yshift=10,\n",
    "            xref='x2', \n",
    "            yref='y2')\n",
    "\n",
    "fig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n",
    "            text=r\"Start Of Incline\",\n",
    "            showarrow=True,\n",
    "            arrowhead=6,\n",
    "            yshift=10,\n",
    "            xref='x2', \n",
    "            yref='y2')\n",
    "\n",
    "fig['layout']['xaxis2']['title'] = 'Date'\n",
    "fig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e29dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8487b8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60886561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbfcfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c6e6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9acaf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baab718",
   "metadata": {},
   "source": [
    "I will apply stop words to the column text to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1927eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72435dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].str.lower()\n",
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114af894",
   "metadata": {},
   "source": [
    "I will now clean and remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1534e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['text']= df['text'].apply(lambda x: remove_punctuations(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84446ff8",
   "metadata": {},
   "source": [
    "Cleaning and removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd0a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df['text'] = df['text'].apply(lambda x: remove_repeating_char(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93332ea8",
   "metadata": {},
   "source": [
    "Removing URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URLs(df):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',df)\n",
    "df['text'] = df['text'].apply(lambda x: remove_URLs(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202c95d",
   "metadata": {},
   "source": [
    " Cleaning and removing numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(df):\n",
    "    return re.sub('[0-9]+', '', df)\n",
    "df['text'] = df['text'].apply(lambda x: remove_numbers(x))\n",
    "df['text'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247953b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a4ec2",
   "metadata": {},
   "source": [
    "I have removed all stop words from text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb909d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a955b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = ' '.join(df[df['sentiment'] == 1]['text'].str.lower())\n",
    "negative_tweets = ' '.join(df[df['sentiment'] == 0]['text'].str.lower())\n",
    "neutral_tweets = ' '.join(df[df['sentiment'] == -1]['text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f2b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308e30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(Positive_Sentiment)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Positive tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(stopwords = STOPWORDS, background_color = \"white\", max_words = 1000).generate(negative_tweets)\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Neutral tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee7dd2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
